{"cells":[{"cell_type":"markdown","metadata":{"id":"aWuBy5CpTM4G"},"source":["# **Create your own layer** - *Inspiré de la formation de Thibault Neveu*\n","We are going to create a custom layer. The goal of this layer is to create a multi layer perceptron. By doing so, we can also learn how to use some low level operation with tensorflow 2.0."]},{"cell_type":"markdown","source":["# 0. Imports"],"metadata":{"id":"22cxTeOkTkKu"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"DQTriTszTM4M","executionInfo":{"status":"ok","timestamp":1736875369841,"user_tz":-60,"elapsed":12836,"user":{"displayName":"Gaby PAULTRE","userId":"12911743808993484272"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"zw_lMvcATM4P"},"source":["# 1. Be sure to use Tensor Flow 2.0"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"bOFUthj8TM4Q","executionInfo":{"status":"ok","timestamp":1736875369844,"user_tz":-60,"elapsed":19,"user":{"displayName":"Gaby PAULTRE","userId":"12911743808993484272"}}},"outputs":[],"source":["assert hasattr(tf, \"function\") # Be sure to use tensorflow 2.0"]},{"cell_type":"markdown","metadata":{"id":"WkKss1AsTM4R"},"source":["# 2. Multi Layer Perceptron\n","\n","This layer has no real purpose and should never be used in production. This is just an example to show how to create a custom layer."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSiNnea0TM4R","executionInfo":{"status":"ok","timestamp":1736875371501,"user_tz":-60,"elapsed":1670,"user":{"displayName":"Gaby PAULTRE","userId":"12911743808993484272"}},"outputId":"79deecb9-18f4-4853-b147-9a65ebe89548"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 882ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[0.5, 0.5],\n","       [0.5, 0.5],\n","       [0.5, 0.5],\n","       [0.5, 0.5],\n","       [0.5, 0.5]], dtype=float32)"]},"metadata":{},"execution_count":3}],"source":["class MlpLayer(tf.keras.layers.Layer):\n","\n","    def __init__(self, units, activations, **kwargs):\n","        super(MlpLayer, self).__init__(**kwargs)\n","        # Set the property to the layer\n","        self.units = units\n","        self.activations_list = activations\n","        self.weights_list = []\n","\n","    # The build method will be called once\n","    # we know the shape of the previous Layer: input_shape\n","    def build(self, input_shape):\n","        # Create trainable weights variables for this layer.\n","        # We create matrix of weights for each layer\n","        # Each weight have this shape: (previous_layer_size, layer_size)\n","        i = 0\n","        for units in self.units:\n","            weights = self.add_weight(\n","                        name='weights-%s' % i,\n","                        shape=(input_shape[1], units),\n","                        initializer='uniform',\n","                        trainable=True\n","            )\n","            i += 1\n","            self.weights_list.append(weights)\n","            input_shape = (None, units)\n","        super(MlpLayer, self).build(input_shape)\n","\n","    def call(self, x):\n","        output = x\n","\n","        # We go through each weight to compute the dot product between the previous\n","        # activation and the weight of the layer.\n","        # At the first pass, the previous activation is just the variable \"x\": The input vector\n","        for weights, activation in zip(self.weights_list, self.activations_list):\n","            # We can still used low level operations as tf.matmul, tf.nn.relu...\n","            output = tf.matmul(output, weights)\n","\n","            if activation == \"relu\":\n","                output = tf.nn.relu(output)\n","            elif activation == \"sigmoid\":\n","                output = tf.nn.sigmoid(output)\n","            elif activation == \"softmax\":\n","                output = tf.nn.softmax(output)\n","\n","        return output\n","\n","    # By adding the get_config method you can then save your model with the custom layer\n","    # and retrieve the model with the same parameters\n","    def get_config(self):\n","        config = {\n","            'units': self.units,\n","            \"activations\": self.activations_list\n","        }\n","        # Retrieve the config from the parent layer\n","        base_config = super(MlpLayer, self).get_config()\n","        # Return the final config\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","# Flatten\n","model = tf.keras.models.Sequential()\n","\n","# Add the layers\n","model.add(MlpLayer([4 , 2], [\"relu\", \"softmax\"]))\n","model.predict(np.zeros((5, 10)))"]},{"cell_type":"markdown","source":["Pour mieux comprendre comment l'exemple fourni fonctionne, examinons pas à pas ce qui se passe lorsque vous utilisez la couche personnalisée MlpLayer dans un modèle séquentiel et faites une prédiction sur des données d'entrée.\n","\n","----------------------------------------\n","\n","Mise en place de l'exemple\n","\n","Création du modèle séquentiel\n","\n","model = tf.keras.models.Sequential()\n","Un modèle Keras séquentiel est créé, ce qui signifie que les couches seront empilées linéairement les unes après les autres.\n","Ajout de la couche personnalisée MlpLayer\n","\n","model.add(MlpLayer([4, 2], [\"relu\", \"softmax\"]))\n","La couche personnalisée MlpLayer est ajoutée au modèle avec deux sous-couches :\n","Première sous-couche : 4 neurones avec la fonction d'activation ReLU.\n","Deuxième sous-couche : 2 neurones avec la fonction d'activation Softmax.\n","Prédiction sur un échantillon d'entrée\n","\n","model.predict(np.zeros((5, 10)))\n","Le modèle effectue une prédiction sur un lot d'entrée de forme (5, 10), ce qui signifie 5 exemples, chacun avec 10 caractéristiques.\n","Déroulement des calculs\n","Initialisation et construction des poids\n","Initialisation (__init__) :\n","\n","units = [4, 2] et activations = [\"relu\", \"softmax\"] sont définis.\n","weights_list est initialisé en tant que liste vide pour stocker les matrices de poids de chaque sous-couche.\n","Construction (build) :\n","\n","----------------------------------------\n","\n","\n","Pour la première sous-couche (4 neurones) :\n","La taille d'entrée est (10,) (10 caractéristiques).\n","Une matrice de poids est créée de forme (10, 4), initialisée uniformément.\n","Ces poids sont ajoutés à weights_list.\n","La taille de sortie est mise à jour à (4,).\n","Pour la deuxième sous-couche (2 neurones) :\n","La taille d'entrée devient (4,) (4 caractéristiques, sortie de la première sous-couche).\n","Une matrice de poids est créée de forme (4, 2), initialisée uniformément.\n","Ces poids sont ajoutés à weights_list.\n","Passe avant (call)\n","Entrée Initiale :\n","\n","L'entrée x a la forme (5, 10), soit 5 exemples avec 10 caractéristiques chacun, initialisées à zéro.\n","Première Sous-couche :\n","\n","Produit Matriciel : output = tf.matmul(x, weights_1) où weights_1 a la forme (10, 4).\n","Chaque exemple est transformé en un vecteur de 4 valeurs à l'aide des poids.\n","Activation ReLU : output = tf.nn.relu(output)\n","Applique ReLU, transformant les valeurs négatives en zéro, pour chaque élément du vecteur de sortie.\n","Deuxième Sous-couche :\n","\n","Produit Matriciel : output = tf.matmul(output, weights_2) où weights_2 a la forme (4, 2).\n","Le vecteur de 4 valeurs est transformé en un vecteur de 2 valeurs.\n","Activation Softmax : output = tf.nn.softmax(output)\n","Convertit le vecteur de sortie en une distribution de probabilité (somme à 1).\n","Résultat de la Prédiction\n","Le modèle renvoie une matrice de sortie de forme (5, 2), où chaque ligne représente un exemple et chaque colonne représente la probabilité d'appartenance à l'une des deux classes.\n","\n","\n","----------------------------------------\n","\n","Cet exemple montre comment la couche MlpLayer prend une entrée de 10 caractéristiques, la transforme à travers deux sous-couches définies par les paramètres [4, 2] et les fonctions d'activation [\"relu\", \"softmax\"], et produit une sortie qui est une distribution de probabilité sur deux classes pour chaque exemple. Chaque étape, de l'initialisation à la passe avant, est soigneusement orchestrée pour respecter les dimensions et appliquer les transformations souhaitées."],"metadata":{"id":"Y8vKHE_zYuSZ"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RBmyHsrDTM4V","executionInfo":{"status":"ok","timestamp":1736875371502,"user_tz":-60,"elapsed":38,"user":{"displayName":"Gaby PAULTRE","userId":"12911743808993484272"}},"outputId":"61bd241e-f81a-460d-d99d-bf8c6559b363"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]}],"source":["model.save(\"custom_layer_in_model.h5\")\n","\n","model.save(\"custom_layer_in_model.keras\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KHbfd-sjTM4W","executionInfo":{"status":"ok","timestamp":1736875371502,"user_tz":-60,"elapsed":29,"user":{"displayName":"Gaby PAULTRE","userId":"12911743808993484272"}},"outputId":"6d69f8dd-e997-4821-c641-6b746e0b6d99"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]}],"source":["custom_objects={'MlpLayer': MlpLayer}\n","loaded_model = tf.keras.models.load_model(\"custom_layer_in_model.h5\", custom_objects=custom_objects)"]},{"cell_type":"code","source":["# Chargement du modèle depuis le fichier .keras\n","loaded_model = tf.keras.models.load_model(\"custom_layer_in_model.keras\", custom_objects={'MlpLayer': MlpLayer})\n","\n","# Compiler le modèle après le chargement si vous prévoyez de continuer l'entraînement\n","loaded_model.compile(\n","    loss=\"sparse_categorical_crossentropy\",\n","    optimizer=\"sgd\",\n","    metrics=[\"accuracy\"]\n",")"],"metadata":{"id":"D3Cu5Kp7aAup","executionInfo":{"status":"ok","timestamp":1736875371992,"user_tz":-60,"elapsed":510,"user":{"displayName":"Gaby PAULTRE","userId":"12911743808993484272"}}},"execution_count":6,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}